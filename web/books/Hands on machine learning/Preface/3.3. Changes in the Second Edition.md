# **Changes in the Second Edition**

This second edition has five main objectives:

1. Cover additional topics: additional unsupervised learning techniques (including clustering, anomaly detection, density estimation and mixture models), additional techniques for training deep nets (including self-normalized networks), additional computer vision techniques (including the Xception, SENet, object
detection with YOLO, and semantic segmentation using R-CNN), handling sequences using CNNs (including WaveNet), natural language processing using RNNs, CNNs and Transformers, generative adversarial networks, deploying TensorFlow models, and more.

2. Update the book to mention some of the latest results from Deep Learning research.

3. Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s implementation of the Keras API (called tf.keras) whenever possible, to simplify the code examples.

4. Update the code examples to use the latest version of Scikit-Learn, NumPy, Pandas, Matplotlib and other libraries.

5. Clarify some sections and fix some errors, thanks to plenty of great feedback from readers.

Some chapters were added, others were rewritten and a few were reordered. ***Table P-1*** shows the mapping between the 1st edition chapters and the 2nd edition chapters:

<em>***Table P-1.*** Chapter mapping between 1st and 2nd edition</em>

| 1st Ed.chapter    | 2nd Ed.chapter      | % Changes      | 2nd Ed.chapter                     
| ----------------- | ------------------- |----------------|----------------------- 
| 1                 | 1                   | <10%           | The Machine Learning Landscape
| 2                 | 2                   | <10%           | End-to-End Machine Learning Project
| 3                 | 3                   | <10%           | Classification
| 4                 | 4                   | <10%           | Training Models
| 5                 | 5                   | <10%           | Support Vector Machines
| 6                 | 6                   | <10%           | Decision Trees
| 7                 | 7                   | <10%           | Ensemble Learning and Random Forests
| 8                 | 8                   | <10%           | Dimensionality Reduction
| N/A               | 9                   | 100% new       | Unsupervised Learning Techniques
| 10                | 10                  | ~75%           | Introduction to Artificial Neural Networks with Keras
| 11                | 11                  | ~50%           | Training Deep Neural Networks
| 9                 | 12                  | 100% rewritten | Custom Models and Training with TensorFlow
| Part of 12        | 13                  | 100% rewritten | Loading and Preprocessing Data with TensorFlow
| 13                | 14                  | ~50%           | Deep Computer Vision Using Convolutional Neural Networks
| Part of 14        | 15                  | ~75%           | Processing Sequences Using RNNs and CNNs
| Part of 14        | 16                  | ~90%           | Natural Language Processing with RNNs and Attention
| 15                | 17                  | ~75%           | Autoencoders and GANs
| 16                | 18                  | ~75%           | Reinforcement Learning
| Part of 12        | 19                  | 100% rewritten | Deploying your TensorFlow Models

More specifically, here are the main changes for each 2nd edition chapter (other than clarifications, corrections and code updates):

- Chapter 1
     * Added a section on handling mismatch between the training set and the validation & test sets.
    
- Chapter 2
    *  Added how to compute a confidence interval.
    *  Improved the installation instructions (e.g., for Windows).
    *  Introduced the upgraded OneHotEncoder and the new ColumnTransformer.

- Chapter 4
    *  Explained the need for training instances to be Independent and Identically Distributed (IID).

- Chapter 7
    *  Added a short section about XGBoost.

- Chapter 9 – new chapter including:
    *  Clustering with K-Means, how to choose the number of clusters, how to use it for dimensionality reduction, semi-supervised learning, image segmentation, and more.
    *  Gaussian mixture models, the Expectation-Maximization (EM) algorithm, Bayesian variational inference, and how mixture models can be used for clustering, density estimation, anomaly detection and novelty detection.
    *  Overview of other anomaly detection and novelty detection algorithms.

- Chapter 10 (mostly new)
    *  Added an introduction to the Keras API, including all its APIs (Sequential, Functional and Subclassing), persistence and callbacks (including the Tensor Board callback).

- Chapter 11 (many changes)
    *  Introduced self-normalizing nets, the SELU activation function and Alpha Dropout.
    *  Introduced self-supervised learning.
    *  Added Nadam optimization.
    *  Added Monte-Carlo Dropout.
    *  Added a note about the risks of adaptive optimization methods.
    *  Updated the practical guidelines.

- Chapter 12 – completely rewritten chapter, including:
    *  A tour of TensorFlow 2
    *  TensorFlow’s lower-level Python API
    *  Writing custom loss functions, metrics, layers, models
    *  Using auto-differentiation and creating custom training algorithms.
    *  TensorFlow Functions and graphs (including tracing and autograph).

- Chapter 13 – new chapter, including:
    *  The Data API
    *  Loading/Storing data efficiently using TFRecords
    *  The Features API (including an introduction to embeddings).
    *  An overview of TF Transform and TF Datasets
    *  Moved the low-level implementation of the neural network to the exercises.
    *  Removed details about queues and readers that are now superseded by the Data API.

- Chapter 14
    *  Added Xception and SENet architectures.
    *  Added a Keras implementation of ResNet-34.
    *  Showed how to use pretrained models using Keras.
    *  Added an end-to-end transfer learning example.
    *  Added classification and localization.
    *  Introduced Fully Convolutional Networks (FCNs).
    *  Introduced object detection using the YOLO architecture.
    *  Introduced semantic segmentation using R-CNN.

- Chapter 15
    *  Added an introduction to Wavenet.
    *  Moved the Encoder–Decoder architecture and Bidirectional RNNs to Chapter 16.

- Chapter 16
    *  Explained how to use the Data API to handle sequential data.
    *  Showed an end-to-end example of text generation using a Character RNN, using both a stateless and a stateful RNN.
    *  Showed an end-to-end example of sentiment analysis using an LSTM.
    *  Explained masking in Keras.
    *  Showed how to reuse pretrained embeddings using TF Hub.
    *  Showed how to build an Encoder–Decoder for Neural Machine Translation using TensorFlow Addons/seq2seq.
    *  Introduced beam search.
    *  Explained attention mechanisms.
    *  Added a short overview of visual attention and a note on explainability.
    *  Introduced the fully attention-based Transformer architecture, including positional embeddings and multi-head attention.
    *  Added an overview of recent language models (2018).

- Chapters 17, 18 and 19: coming soon.


















